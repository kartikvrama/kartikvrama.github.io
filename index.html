<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');

    function toggleblock(blockId) {
      var block = document.getElementById(blockId);
      if (block.style.display == 'none') {
        block.style.display = 'block';
      } else {
        block.style.display = 'none';
      }
    }

  </script>

  <title>Kartik Ramachandruni</title>

  <meta name="author" content="Kartik R">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kartik Ramachandruni</name>
                  </p>
                  <p>I am a PhD candidate in Robotics
                    <a href="https://ic.gatech.edu/">(Interactive Computing department)</a>
                    at the Georgia Institute of Technology</a>. I am advised by
                    <a href="https://www.cc.gatech.edu/~chernova/">Prof. Sonia Chernova</a>
                    as part of the <a href="http://rail.gatech.edu/">Robot Autonomy and Interactive Learning (RAIL)
                      lab</a>. My research is dedicated to developing adaptive household robots that collaborate with
                      and assists users in their daily lives. My doctoral work focuses on two core areas: adaptive human-robot collaboration, where the robot complements human behavior using implicit cues; and personalized object rearrangement, where the robot infers user preferences from past observations of object arrangements without explicit user instruction.
                  </p>

                  <p>
                    <strong>Previously:</strong> Student Researcher at Google Cerebra, NY, Robotics Researcher at TCS Research and Innovation Labs, India
                  </p>

                  </Tatap>
                  <div class="social-buttons-container">
                    <a href="mailto:kvr6@gatech.edu" class="social-button">
                      <svg class="social-button-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
                      </svg>
                      Email
                    </a>
                    <a href="https://www.linkedin.com/in/kartik-ramachandruni" class="social-button">
                      <svg class="social-button-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                      </svg>
                      LinkedIn
                    </a>
                    <a href="https://scholar.google.com/citations?user=c6OhZroAAAAJ&hl=en&oi=ao" class="social-button">
                      <svg class="social-button-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/>
                      </svg>
                      Google Scholar
                    </a>
                  </div>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/headshot-side.jpg"><img alt="profile photo"
                      src="images/headshot-side.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <p style="text-align:left; font-size: 18px;">
            <strong style="color:#217a36; font-size: 1em;">I am actively seeking full-time academic and industry research positions starting in April or May 2026!</strong>
          </p>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div class="publications-heading">Publications</div>
                </td>
              </tr>
              
              <tr class="publication-row">
                <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:top">
                  <div class="publication-box">
                    <div class="publication-title">
                      <a href="https://arxiv.org/abs/2505.11108" style="color:#333;text-decoration:none;">
                        Personalized Robotic Object Rearrangement from Scene Context
                      </a>
                    </div>
                    <div class="publication-authors">
                      <strong>Kartik Ramachandruni</strong>,
                      <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                    </div>
                    <div class="publication-venue">
                      RO-MAN 2025, ICRA HCRL 2025 Workshop
                    </div>
                    <div class="publication-abstract" id="parsec_2025abs" style="display: none;">
                      Object rearrangement is a key task for household robots requiring personalization without explicit instructions, meaningful object placement in environments occupied with objects, and generalization to unseen objects and new environments. To facilitate research addressing these challenges, we introduce PARSEC, an object rearrangement benchmark for learning user organizational preferences from observed scene context to place objects in a partially arranged environment. PARSEC is built upon a novel dataset of 110K rearrangement examples crowdsourced from 72 users, featuring 93 object categories and 15 environments. To better align with real-world organizational habits, we propose ContextSortLM, an LLM-based personalized rearrangement model that handles flexible user preferences by explicitly accounting for objects with multiple valid placement locations when placing items in partially arranged environments. We evaluate ContextSortLM and existing personalized rearrangement approaches on the PARSEC benchmark and complement these findings with a crowdsourced evaluation of 108 online raters ranking model predictions based on alignment with user preferences. Our results indicate that personalized rearrangement models leveraging multiple scene context sources perform better than models relying on a single context source. Moreover, ContextSortLM outperforms other models in placing objects to replicate the target user's arrangement and ranks among the top two in all three environment categories, as rated by online evaluators. Importantly, our evaluation highlights challenges associated with modeling environment semantics across different environment categories and provides recommendations for future work.
                    </div>
                    <div class="publication-links">
                      <a href="https://arxiv.org/abs/2505.11108">Paper</a> | 
                      <a href="https://github.com/kartikvrama/parsec/">Code</a> | 
                      <a href="javascript:toggleblock('parsec_2025abs')">Abstract</a>
                    </div>
                  </div>
                </td>
              </tr>

              <tr class="publication-row">
                <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:top">
                  <div class="publication-box">
                    <div class="publication-title">
                      <a href="https://arxiv.org/abs/2310.00371" style="color:#333;text-decoration:none;">
                        ConSOR: A Context-Aware Semantic Object Rearrangement Framework
                        for Partially Arranged Scenes
                      </a>
                    </div>
                    <div class="publication-authors">
                      <strong>Kartik Ramachandruni</strong>,
                      <a href="https://maxzuo.ai/">Max Zuo</a>,
                      <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                    </div>
                    <div class="publication-venue">
                      International Conference on Intelligent Robots and Systems (IROS) 2023, IROS ARC 2023 workshop
                    </div>
                    <div class="publication-abstract" id="iros_2023abs" style="display: none;">
                      Object rearrangement is the problem of enabling a robot to identify the correct object placement
                      in a complex environment. Prior work on object rearrangement has explored a diverse set of
                      techniques
                      for following user instructions to achieve some desired goal state. Logical predicates, images of
                      the
                      goal scene, and natural language descriptions have all been used to instruct a robot in how to
                      arrange
                      objects. In this work, we argue that burdening the user with specifying goal scenes is not
                      necessary in partially-arranged environments, such as common household settings. Instead, we show
                      that
                      contextual cues from partially arranged scenes (i.e., the placement of some number of pre-arranged
                      objects in
                      the environment) provide sufficient context to enable robots to perform object rearrangement
                      \textit{without any explicit user goal specification}. We introduce ConSOR, a Context-aware
                      Semantic Object
                      Rearrangement framework that utilizes contextual cues from a partially arranged initial state of
                      the environment to complete the arrangement of new objects, without explicit goal specification
                      from
                      the user. We demonstrate that ConSOR strongly outperforms two baselines in generalizing to novel
                      object arrangements and unseen object categories. The code and data are available at <a
                        href="https://github.com/kartikvrama/consor">https://github.com/kartikvrama/consor
                      </a>.
                    </div>
                    <div class="publication-links">
                      <a href="https://arxiv.org/abs/2310.00371">Paper</a> |
                      <a href="https://github.com/kartikvrama/consor/">Code</a> | 
                      <a href="javascript:toggleblock('iros_2023abs')">Abstract</a> | 
                      <a href="data/2023_IROS_ConSOR_poster.pdf">Poster</a> | 
                      <a href="data/2023_IROS_ARC.pdf">Workshop</a>
                    </div>
                  </div>
                </td>
              </tr>

              <tr class="publication-row">
                <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:top">
                  <div class="publication-box">
                    <div class="publication-title">
                      <a href="https://dl.acm.org/doi/pdf/10.1145/3623387" style="color:#333;text-decoration:none;">
                        UHTP: A User-Aware Hierarchical Task Planning Framework for
                        Communication-Free, Mutually-Adaptive Human-Robot Collaboration
                      </a>
                    </div>
                    <div class="publication-authors">
                      <strong>Kartik Ramachandruni*</strong>,
                      <a href="https://dekent.github.io/">Cassandra Kent*</a>,
                      <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                    </div>
                    <div class="publication-venue">
                      Transactions on Human-Robot Interaction (THRI) 2023, ACC HAI 2022 workshop
                    </div>
                    <div class="publication-abstract" id="thri_uhtp" style="display: none;">
                      Collaborative human-robot task execution approaches require
                      mutual adaptation, allowing both the human and robot partners to
                      take active roles in action selection and role assignment to
                      achieve a single shared goal. Prior works have utilized a
                      leader-follower paradigm in which either agent must follow the
                      actions specified by the other agent. We introduce the User-aware
                      Hierarchical Task Planning (UHTP) framework, a communication-free
                      human-robot collaborative approach for adaptive execution of
                      multi-step tasks that moves beyond the leader-follower paradigm.
                      Specifically, our approach enables the robot to observe the human,
                      perform actions that support the human's decisions, and actively
                      select actions that maximize the expected efficiency of the
                      collaborative task. In turn, the human chooses actions based on
                      their observation of the task and the robot, without being
                      dictated by a scheduler or the robot. We evaluate UHTP both in
                      simulation and in a human subjects experiment of a collaborative
                      drill assembly task. Our results show that UHTP achieves more
                      efficient task plans and shorter task completion times than
                      non-adaptive baselines across a wide range of human behaviors,
                      that interacting with a UHTP-controlled robot reduces the human's
                      cognitive workload, and that humans prefer to work with our
                      adaptive robot over a fixed-policy alternative.
                    </div>
                    <div class="publication-links">
                      <a href="https://dl.acm.org/doi/pdf/10.1145/3623387">Paper</a> |
                      <a href="https://github.com/kartikvrama/uhtp">Code</a> | 
                      <a href="javascript:toggleblock('thri_uhtp')">Abstract</a> | 
                      <a href="data/2022_ACC_HAI2.pdf">Workshop</a>
                    </div>
                  </div>
                </td>
              </tr>

              <tr class="publication-row">
                <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:top">
                  <div class="publication-box">
                    <div class="publication-title">
                      <a href="https://www.sciencedirect.com/science/article/pii/S092188902200183X" style="color:#333;text-decoration:none;">
                        A Survey of Semantic Reasoning Frameworks for Robotic Systems
                      </a>
                    </div>
                    <div class="publication-authors">
                      <a href="http://weiyuliu.com/">Weiyu Liu*</a>,
                      <a href="https://adaruna3.github.io/adaruna3/">Angel Daruna*</a>,
                      <strong>Kartik Ramachandruni</strong>**,
                      <a href="https://maithili.github.io/">Maithili Patel**</a>,
                      <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                    </div>
                    <div class="publication-venue">
                      Robotics and Autonomous Systems (RAS) 2023
                    </div>
                    <div class="publication-abstract" id="ras_2023abs" style="display: none;">
                      Robots are increasingly transitioning from specialized, single-task machines to general-purpose
                      systems that operate in diverse and dynamic environments. To address the challenges associated
                      with operation in real-world domains, robots must effectively generalize knowledge, learn, and be
                      transparent in their decision making. This survey examines Semantic Reasoning techniques for
                      robotic systems, which enable robots to encode and use semantic knowledge, including concepts,
                      facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing
                      semantic knowledge allows a robot to identify the meaningful patterns shared between problems and
                      environments, and therefore more effectively perform a wide range of real-world tasks. We identify
                      the three common components that make up a computational Semantic Reasoning Framework: knowledge
                      sources, computational frameworks, and world representations. We analyze the existing
                      implementations and the key characteristics of these components, highlight the many interactions
                      that occur between them, and examine their integration for solving robotic tasks related to five
                      aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the
                      computational formulation and underlying mechanisms of existing methods, we provide a unified view
                      of the wide range of semantic reasoning techniques and identify open areas for future
                      research.
                    </div>
                    <div class="publication-links">
                      <a href="https://www.sciencedirect.com/science/article/pii/S092188902200183X">Paper</a> | 
                      <a href="javascript:toggleblock('ras_2023abs')">Abstract</a>
                    </div>
                  </div>
                </td>
              </tr>

              <tr class="publication-row">
                <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:top">
                  <div class="publication-box">
                    <div class="publication-title">
                      <a href="data/ICRA_AT-Net.pdf" style="color:#333;text-decoration:none;">
                        Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using
                        Video Demonstration
                      </a>
                    </div>
                    <div class="publication-authors">
                      <strong>Kartik Ramachandruni</strong>,
                      <a href="https://madhubabuv.github.io/">Madhu Vankadari</a>,
                      <a href="https://scholar.google.co.in/citations?user=yIkPDR0AAAAJ&hl=en">Anima Majumder</a>,
                      <a href="https://scholar.google.co.in/citations?user=-_ELGZcAAAAJ&hl=en">Samrat Dutta</a>,
                      <a href="https://scholar.google.co.in/citations?user=gXORbx0AAAAJ&hl=en">Swagat Kumar</a>
                    </div>
                    <div class="publication-venue">
                      International Conference on Robotics and Automation (ICRA) 2020
                    </div>
                    <div class="publication-abstract" id="icra_20abs" style="display: none;">
                      This paper proposes an end-to-end self-supervised feature representation network named Attentive
                      Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel
                      multi-level spatial attention module to identify the intended task demonstrated by the expert. The
                      neural connections in AT-Net ensure the relevant information in the demonstration is amplified and
                      the irrelevant information is suppressed while learning task-specific feature embeddings. This is
                      achieved by a weighted combination of multiple inter mediate feature maps of the input image at
                      different stages of the CNN pipeline. The weights of the combination are given by the
                      compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is
                      trained using a metric learning loss which aims to decrease the distance between the feature
                      representations of concurrent frames from multiple view points and increase the distance between
                      temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement
                      learning problem for task imitation. Through experiments on the publicly available Multi-view
                      pouring dataset, it is demonstrated that the output of the attention module highlights the
                      task-specific objects while suppressing the rest of the background. The efficacy of the proposed
                      method is further validated by qualitative and quantitative comparison with a state-of-the-art
                      technique along with intensive ablation studies. The proposed method is implemented to imitate a
                      pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show
                      that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of
                      training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the
                      intended task.
                    </div>
                    <div class="publication-links">
                      <a href="data/ICRA_AT-Net.pdf">Paper</a> | 
                      <a href="javascript:toggleblock('icra_20abs')">Abstract</a>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- AIR 2019 WORK -->
              <!-- 
              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="data/AIR_ACM_withDOI.pdf">
                    <papertitle>Vision-based control of UR5 robot to track a moving object under occlusion using Adaptive Kalman Filter</papertitle>
                  </a>
                  <br>
                  <strong>Kartik Ramachandruni</strong>,
                  Shivam Jaiswal,
                  <a href="https://scholar.google.ca/citations?user=9CW-7GAAAAAJ&hl=en">Suril V. Shah</a>
                  <br>
                  <a href="https://advancesinrobotics.com/2019/"><em>Advances In Robotics (AIR) 2019</em></a>
                  <br>
                  <a href="data/AIR_ACM_withDOI.pdf">Paper</a> | <a href="javascript:toggleblock('air_2019abs')">Abstract</a>
                  <br>
                  <p align="justify"> <i id='air_2019abs' style="display: none;">
                    This paper presents a robust method to track a moving object under occlusion using an off-the-shelf monocular camera and a 6 Degree of Freedom (DOF) articulated arm. The visual servoing problem of tracking a known object using data from a monocular camera can be solved with a simple closed loop controller. However, this system frequently fails in situations where the object cannot be detected and to overcome this problem an estimation based tracking system is required. This work employs an Adaptive Kalman Filter (AKF) to improve the visual feedback of the camera. The role of the AKF is to estimate the position of the object when it is occluded/out of view and remove the noise and uncertainties associated with visual data. Two estimation models for the AKF are selected for comparison and among them, the Mean-Adaptive acceleration model is implemented on a 6-DOF UR5 articulated arm with a monocular camera mounted in eye-in-hand configuration to follow the known object in 2D cartesian space (without using depth information).</i></p>
                  <p></p>
                </td>
              </tr>
            -->

              <tr>
                <td>
                  <center>Design and source code from <a href="https://jonbarron.info/">Jon Barron's </a> website
                  </center>
              </tr>
        </td>
    </tbody>
  </table>
  </tbody>
  </table>

</html>