<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');

    function toggleblock(blockId) {
      var block = document.getElementById(blockId);
      if (block.style.display == 'none') {
        block.style.display = 'block';
      } else {
        block.style.display = 'none';
      }
    }
  </script>

  <title>Kartik Ramachandruni</title>

  <meta name="author" content="Kartik R">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kartik Ramachandruni</name>
                  </p>
                  <p>I am a 4<sup>th</sup> Year PhD student in Robotics
                    <a href="https://ic.gatech.edu/">(Interactive Computing department)</a>
                    at the Georgia Institute of Technology</a>. I am advised by
                    <a href="https://www.cc.gatech.edu/~chernova/">Prof. Sonia Chernova</a>
                    as part of the <a href="http://rail.gatech.edu/">Robot Autonomy and Interactive Learning (RAIL)
                      lab</a>. My research focuses on enabling embodied AI agents to assist users with daily routines in unstructured and
                      highly personalized human spaces with minimal task supervision.
                  </p>
                  <p>
                    I graduated from the
                    <a href="http://www.iitj.ac.in/">Indian Institute of Technology Jodhpur</a>
                    with a B.Tech. in Mechanical Engineering, after which I worked
                    as a robotics researcher for two years at the Tata Research and
                    Innovation labs in Bangalore, India. I also did a summer research internship with the Google
                    Cerebra team in New York during the summer of 2023, where I worked on developing
                    uncertainty-aware LLM agents for UI automation tasks.
                  </p>
                  <p>
                    </Tatap>
                  <p style="text-align:center">
                    Email: <a style="color:#0000EE;">kvr6 [at] gatech [dot] edu</a> &nbsp|&nbsp
                    <a href="data/kartik_resume.pdf">Resume/CV</a> &nbsp|&nbsp
                    <a href="https://scholar.google.com/citations?user=c6OhZroAAAAJ&hl=en&oi=ao">Scholar</a>
                    <!-- &nbsp|&nbsp -->
                    <!-- <a href="https://www.linkedin.com/in/kartik-ramachandruni"> LinkedIn </a> -->
                  </p>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/kartik_2024.jpg"><img style="width:130%;max-width:130%" alt="profile photo"
                      src="images/kartik_2024.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I am currently working on user-assistive robots that learn user organizational preferences, such as object placement
                    and arrangement preferences, from passive observation rather than explicit task instructions or demonstrations.
                    My research aims to develop semantic reasoning techniques that can learn novel rearrangement preferences
                    by integrating contextual cues from passive observations of partially arranged environments
                    (e.g. a half-empty fridge or cabinet), with the aim of generalizing to previously unseen objects and households.
                    My broader research interests include employing user interaction to resolve
                    goal uncertainty in assistive tasks, user-adaptive task planning for human-robot collaboration, and robot imitation
                    learning from video demonstrations.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2310.00371">
                    <papertitle>
                      ConSOR: A Context-Aware Semantic Object Rearrangement Framework
                      for Partially Arranged Scenes
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Ramachandruni</strong>,
                  <a href="https://maxzuo.ai/">Max Zuo</a>,
                  <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                  <br>
                  <em>International Conference on Intelligent Robots and Systems (IROS) 2023, IROS ARC 2023
                    workshop</em>
                  <br>
                  <a href="https://arxiv.org/abs/2310.00371">Paper</a> |
                  <a href="https://github.com/kartikvrama/consor/">Code</a> | <a href="javascript:toggleblock('iros_2023abs')">Abstract</a> | <a
                    href="data/2023_IROS_ConSOR_poster.pdf">Poster</a> | <a href="data/2023_IROS_ARC.pdf">Workshop</a>
                  <br>
                  <p align="justify"> <i id='iros_2023abs' style="display: none;">
                      Object rearrangement is the problem of enabling a robot to identify the correct object placement
                      in a complex environment. Prior work on object rearrangement has explored a diverse set of
                      techniques
                      for following user instructions to achieve some desired goal state. Logical predicates, images of
                      the
                      goal scene, and natural language descriptions have all been used to instruct a robot in how to
                      arrange
                      objects. In this work, we argue that burdening the user with specifying goal scenes is not
                      necessary in partially-arranged environments, such as common household settings. Instead, we show
                      that
                      contextual cues from partially arranged scenes (i.e., the placement of some number of pre-arranged
                      objects in
                      the environment) provide sufficient context to enable robots to perform object rearrangement
                      \textit{without any explicit user goal specification}. We introduce ConSOR, a Context-aware
                      Semantic Object
                      Rearrangement framework that utilizes contextual cues from a partially arranged initial state of
                      the environment to complete the arrangement of new objects, without explicit goal specification
                      from
                      the user. We demonstrate that ConSOR strongly outperforms two baselines in generalizing to novel
                      object arrangements and unseen object categories. The code and data are available at <a
                        href="https://github.com/kartikvrama/consor">https://github.com/kartikvrama/consor
                      </a>.
                    </i>
                  </p>
                  <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3623387">
                    <papertitle>
                      UHTP: A User-Aware Hierarchical Task Planning Framework for
                      Communication-Free, Mutually-Adaptive Human-Robot Collaboration
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Ramachandruni*</strong>,
                  <a href="https://dekent.github.io/">Cassandra Kent*</a>,
                  <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                  <br>
                  <em>Transactions on Human-Robot Interaction (THRI) 2023, ACC HAI 2022 workshop</em>
                  <br>
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3623387">Paper | <a
                      href="javascript:toggleblock('thri_uhtp')">Abstract | <a
                        href="data/2022_ACC_HAI2.pdf">Workshop</a>
                      <br>
                      <p align="justify"> <i id='thri_uhtp' style="display: none;">
                          Collaborative human-robot task execution approaches require
                          mutual adaptation, allowing both the human and robot partners to
                          take active roles in action selection and role assignment to
                          achieve a single shared goal. Prior works have utilized a
                          leader-follower paradigm in which either agent must follow the
                          actions specified by the other agent. We introduce the User-aware
                          Hierarchical Task Planning (UHTP) framework, a communication-free
                          human-robot collaborative approach for adaptive execution of
                          multi-step tasks that moves beyond the leader-follower paradigm.
                          Specifically, our approach enables the robot to observe the human,
                          perform actions that support the human's decisions, and actively
                          select actions that maximize the expected efficiency of the
                          collaborative task. In turn, the human chooses actions based on
                          their observation of the task and the robot, without being
                          dictated by a scheduler or the robot. We evaluate UHTP both in
                          simulation and in a human subjects experiment of a collaborative
                          drill assembly task. Our results show that UHTP achieves more
                          efficient task plans and shorter task completion times than
                          non-adaptive baselines across a wide range of human behaviors,
                          that interacting with a UHTP-controlled robot reduces the human's
                          cognitive workload, and that humans prefer to work with our
                          adaptive robot over a fixed-policy alternative.
                        </i>
                      </p>
                      <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.sciencedirect.com/science/article/pii/S092188902200183X">
                    <papertitle>A Survey of Semantic Reasoning Frameworks for Robotic Systems</papertitle>
                  </a>
                  <br>
                  <a href="http://weiyuliu.com/">Weiyu Liu*</a>,
                  <a href="https://adaruna3.github.io/adaruna3/">Angel Daruna*</a>,
                  <strong>Kartik Ramachandruni</strong>**,
                  <a href="https://maithili.github.io/">Maithili Patel**</a>,
                  <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                  <br>
                  <em>Robotics and Autonomous Systems (RAS) 2023</em>
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/pii/S092188902200183X">Paper</a> | <a
                    href="javascript:toggleblock('ras_2023abs')">Abstract</a>
                  <br>
                  <p align="justify"> <i id='ras_2023abs' style="display: none;">
                      Robots are increasingly transitioning from specialized, single-task machines to general-purpose
                      systems that operate in diverse and dynamic environments. To address the challenges associated
                      with operation in real-world domains, robots must effectively generalize knowledge, learn, and be
                      transparent in their decision making. This survey examines Semantic Reasoning techniques for
                      robotic systems, which enable robots to encode and use semantic knowledge, including concepts,
                      facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing
                      semantic knowledge allows a robot to identify the meaningful patterns shared between problems and
                      environments, and therefore more effectively perform a wide range of real-world tasks. We identify
                      the three common components that make up a computational Semantic Reasoning Framework: knowledge
                      sources, computational frameworks, and world representations. We analyze the existing
                      implementations and the key characteristics of these components, highlight the many interactions
                      that occur between them, and examine their integration for solving robotic tasks related to five
                      aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the
                      computational formulation and underlying mechanisms of existing methods, we provide a unified view
                      of the wide range of semantic reasoning techniques and identify open areas for future
                      research.</i></p>
                  <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="data/ICRA_AT-Net.pdf">
                    <papertitle>Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using
                      Video Demonstration</papertitle>
                  </a>
                  <br>
                  <strong>Kartik Ramachandruni</strong>,
                  <a href="https://madhubabuv.github.io/">Madhu Vankadari</a>,
                  <a href="https://scholar.google.co.in/citations?user=yIkPDR0AAAAJ&hl=en">Anima Majumder</a>,
                  <a href="https://scholar.google.co.in/citations?user=-_ELGZcAAAAJ&hl=en">Samrat Dutta</a>,
                  <a href="https://scholar.google.co.in/citations?user=gXORbx0AAAAJ&hl=en">Swagat Kumar</a>
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA) 2020 </em>
                  <br>
                  <a href="data/ICRA_AT-Net.pdf">Paper</a> | <a href="javascript:toggleblock('icra_20abs')">Abstract</a>
                  <br>
                  <p align="justify"> <i id='icra_20abs' style="display: none;">
                      This paper proposes an end-to-end self-supervised feature representation network named Attentive
                      Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel
                      multi-level spatial attention module to identify the intended task demonstrated by the expert. The
                      neural connections in AT-Net ensure the relevant information in the demonstration is amplified and
                      the irrelevant information is suppressed while learning task-specific feature embeddings. This is
                      achieved by a weighted combination of multiple inter mediate feature maps of the input image at
                      different stages of the CNN pipeline. The weights of the combination are given by the
                      compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is
                      trained using a metric learning loss which aims to decrease the distance between the feature
                      representations of concurrent frames from multiple view points and increase the distance between
                      temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement
                      learning problem for task imitation. Through experiments on the publicly available Multi-view
                      pouring dataset, it is demonstrated that the output of the attention module highlights the
                      task-specific objects while suppressing the rest of the background. The efficacy of the proposed
                      method is further validated by qualitative and quantitative comparison with a state-of-the-art
                      technique along with intensive ablation studies. The proposed method is implemented to imitate a
                      pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show
                      that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of
                      training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the
                      intended task.</i></p>
                </td>
              </tr>

              <!-- AIR 2019 WORK -->
              <!-- 
              <tr>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="data/AIR_ACM_withDOI.pdf">
                    <papertitle>Vision-based control of UR5 robot to track a moving object under occlusion using Adaptive Kalman Filter</papertitle>
                  </a>
                  <br>
                  <strong>Kartik Ramachandruni</strong>,
                  Shivam Jaiswal,
                  <a href="https://scholar.google.ca/citations?user=9CW-7GAAAAAJ&hl=en">Suril V. Shah</a>
                  <br>
                  <a href="https://advancesinrobotics.com/2019/"><em>Advances In Robotics (AIR) 2019</em></a>
                  <br>
                  <a href="data/AIR_ACM_withDOI.pdf">Paper</a> | <a href="javascript:toggleblock('air_2019abs')">Abstract</a>
                  <br>
                  <p align="justify"> <i id='air_2019abs' style="display: none;">
                    This paper presents a robust method to track a moving object under occlusion using an off-the-shelf monocular camera and a 6 Degree of Freedom (DOF) articulated arm. The visual servoing problem of tracking a known object using data from a monocular camera can be solved with a simple closed loop controller. However, this system frequently fails in situations where the object cannot be detected and to overcome this problem an estimation based tracking system is required. This work employs an Adaptive Kalman Filter (AKF) to improve the visual feedback of the camera. The role of the AKF is to estimate the position of the object when it is occluded/out of view and remove the noise and uncertainties associated with visual data. Two estimation models for the AKF are selected for comparison and among them, the Mean-Adaptive acceleration model is implemented on a 6-DOF UR5 articulated arm with a monocular camera mounted in eye-in-hand configuration to follow the known object in 2D cartesian space (without using depth information).</i></p>
                  <p></p>
                </td>
              </tr>
            -->

              <tr>
                <td>
                  <center>Design and source code from <a href="https://jonbarron.info/">Jon Barron's </a> website
                  </center>
              </tr>
        </td>
    </tbody>
  </table>
  </tbody>
  </table>

</html>