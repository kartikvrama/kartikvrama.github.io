<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');

    function toggleblock(blockId)
    {
      var block = document.getElementById(blockId);
      if (block.style.display == 'none') {
        block.style.display = 'block' ;
        } else {
        block.style.display = 'none' ;
        }
    }
  </script>

  <title>Kartik Ramachandruni</title>

  <meta name="author" content="Kartik R">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kartik Ramachandruni</name>
              </p>
              <p>I am a 3<sup>rd</sup> Year PhD student in Robotics <a href="https://ic.gatech.edu/">(Interactive Computing department)</a> at the Georgia Institute of Technology</a>. I am advised by <a href="https://www.cc.gatech.edu/~chernova/">Prof. Sonia Chernova</a> as part of the <a href="http://rail.gatech.edu/">Robot Autonomy and Interactive Learning (RAIL) lab</a>. My long-term research interest lies in enabling robot agents to execute real-world tasks without user instruction in novel, unstructured environments.
              </p>
              <p>
                I graduated from the <a href="http://www.iitj.ac.in/">Indian Institute of Technology Jodhpur</a> with a B.Tech. in Mechanical Engineering, after which I worked as a robotics researcher for two years at the Tata Research and Innovation labs in Bangalore, India. 
              </p>
              <center><b>I am actively looking for research internships in Summer 2023!</b></center>
              <p>
              </Tatap>
              <p style="text-align:center">
                <a href="mailto:kvr6@gatech.edu">Email</a> &nbsp|&nbsp
                <a href="data/kartik_resume.pdf">Resume/CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=c6OhZroAAAAJ&hl=en&oi=ao">Scholar</a> <!-- &nbsp|&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/kartik-ramachandruni"> LinkedIn </a> -->
              </p>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/kartik_vertical.png"><img style="width:130%;max-width:130%" alt="profile photo" src="images/kartik_vertical.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I am currently working on a semantic rearrangement framework that determines a tidied environment configuration from partially arranged states without explicit goal specification. In addition to extending my current work, I am highly interested in developing generalizable semantic reasoning frameworks for other real-world long-horizon robot tasks. I have previously worked on research projects in human-robot collaborative task planning and vision-based imitation learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S092188902200183X">
                <papertitle>A survey of Semantic Reasoning frameworks for robotic systems</papertitle>
              </a>
              <br>
              <a href="http://weiyuliu.com/">Weiyu Liu</a>,
              <a href="https://adaruna3.github.io/adaruna3/">Angel Daruna</a>,
              <strong>Kartik Ramachandruni</strong>**,
              <a href="https://maithili.github.io/">Maithili Patel**</a>,
              <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
              <br>
              <em>Robotics and Autonomous Systems (RAS) 2023</em>
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S092188902200183X">Paper</a> | <a href="javascript:toggleblock('ras_2023abs')">Abstract</a>
              <br>
              <p align="justify"> <i id='ras_2023abs' style="display: none;">
		            Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in diverse and dynamic environments. To address the challenges associated with operation in real-world domains, robots must effectively generalize knowledge, learn, and be transparent in their decision making. This survey examines Semantic Reasoning techniques for robotic systems, which enable robots to encode and use semantic knowledge, including concepts, facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing semantic knowledge allows a robot to identify the meaningful patterns shared between problems and environments, and therefore more effectively perform a wide range of real-world tasks. We identify the three common components that make up a computational Semantic Reasoning Framework: knowledge sources, computational frameworks, and world representations. We analyze the existing implementations and the key characteristics of these components, highlight the many interactions that occur between them, and examine their integration for solving robotic tasks related to five aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the computational formulation and underlying mechanisms of existing methods, we provide a unified view of the wide range of semantic reasoning techniques and identify open areas for future research.</i></p>
              <p></p>
          </td></tr>

		        <tr>
		        <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/ICRA_AT-Net.pdf">
		        	  <papertitle>Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration</papertitle>
              </a>
              <br>
			        <strong>Kartik Ramachandruni</strong>,
			        <a href="https://madhubabuv.github.io/">Madhu Vankadari</a>,
			        <a href="https://scholar.google.co.in/citations?user=yIkPDR0AAAAJ&hl=en">Anima Majumder</a>,
			        <a href="https://scholar.google.co.in/citations?user=-_ELGZcAAAAJ&hl=en">Samrat Dutta</a>,
			        <a href="https://scholar.google.co.in/citations?user=gXORbx0AAAAJ&hl=en">Swagat Kumar</a>
			        <br>
			        <em>International Conference on Robotics and Automation (ICRA) 2020 </em> 
              <br>
              <a href="data/ICRA_AT-Net.pdf">Paper</a> | <a href="javascript:toggleblock('icra_20abs')">Abstract</a>
			        <br>
		         	<p align="justify"> <i id='icra_20abs' style="display: none;">
		            This paper proposes an end-to-end self-supervised feature representation network named Attentive Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel multi-level spatial attention module to identify the intended task demonstrated by the expert. The neural connections in AT-Net ensure the relevant information in the demonstration is amplified and the irrelevant information is suppressed while learning task-specific feature embeddings. This is achieved by a weighted combination of multiple inter mediate feature maps of the input image at different stages of the CNN pipeline. The weights of the combination are given by the compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is trained using a metric learning loss which aims to decrease the distance between the feature representations of concurrent frames from multiple view points and increase the distance between temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement learning problem for task imitation. Through experiments on the publicly available Multi-view pouring dataset, it is demonstrated that the output of the attention module highlights the task-specific objects while suppressing the rest of the background. The efficacy of the proposed method is further validated by qualitative and quantitative comparison with a state-of-the-art technique along with intensive ablation studies. The proposed method is implemented to imitate a pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the intended task.</i></p>
		        </td></tr>	

            <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/AIR_ACM_withDOI.pdf">
                <papertitle>Vision-based control of UR5 robot to track a moving object under occlusion using Adaptive Kalman Filter</papertitle>
              </a>
              <br>
              <strong>Kartik Ramachandruni</strong>,
              Shivam Jaiswal,
              <a href="https://scholar.google.ca/citations?user=9CW-7GAAAAAJ&hl=en">Suril V. Shah</a>
              <br>
              <a href="https://advancesinrobotics.com/2019/"><em>Advances In Robotics (AIR) 2019</em></a>
              <br>
              <a href="data/AIR_ACM_withDOI.pdf">Paper</a> | <a href="javascript:toggleblock('air_2019abs')">Abstract</a>
              <br>
              <p align="justify"> <i id='air_2019abs' style="display: none;">
		            This paper presents a robust method to track a moving object under occlusion using an off-the-shelf monocular camera and a 6 Degree of Freedom (DOF) articulated arm. The visual servoing problem of tracking a known object using data from a monocular camera can be solved with a simple closed loop controller. However, this system frequently fails in situations where the object cannot be detected and to overcome this problem an estimation based tracking system is required. This work employs an Adaptive Kalman Filter (AKF) to improve the visual feedback of the camera. The role of the AKF is to estimate the position of the object when it is occluded/out of view and remove the noise and uncertainties associated with visual data. Two estimation models for the AKF are selected for comparison and among them, the Mean-Adaptive acceleration model is implemented on a 6-DOF UR5 articulated arm with a monocular camera mounted in eye-in-hand configuration to follow the known object in 2D cartesian space (without using depth information).</i></p>
              <p></p>
          </td></tr>

          <tr><td><center>Design and source code from <a href="https://jonbarron.info/">Jon Barron's </a> website</center></tr></td>
        </tbody></table>
    </tbody></table>

</html>
